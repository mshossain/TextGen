{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59620c0-f546-42c3-b6b7-4c2b099374d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edf01b2-138b-431f-a501-60a0c3fe93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading text file\n",
    "with open('input_text.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f06bf2-e03b-48d8-befd-2352a99416d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words.append('<eos>')  # Add end-of-sequence token at the end of the text\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(sorted_vocab, 1)}\n",
    "    vocab_to_int['<eos>'] = len(vocab_to_int) + 1  # Add <eos> token\n",
    "    \n",
    "    int_to_vocab = {ii: word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    encoded = [vocab_to_int[word] for word in words]\n",
    "    return encoded, vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ec6830-f87a-4971-8932-eec48e385a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocess the text again\n",
    "encoded_text, vocab_to_int, int_to_vocab = preprocess(text)\n",
    "vocab_size = len(vocab_to_int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71854f31-3c2c-49b4-82e8-d7ec374de08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(encoded, sequence_length, batch_size):\n",
    "    total_length = len(encoded)\n",
    "    n_batches = total_length // (batch_size * sequence_length)\n",
    "    encoded = encoded[:n_batches * batch_size * sequence_length]\n",
    "    input_data = np.array(encoded)\n",
    "    target_data = np.roll(input_data, -1)\n",
    "    \n",
    "    inputs = input_data.reshape((batch_size, -1))\n",
    "    targets = target_data.reshape((batch_size, -1))\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ccd24ea-396f-471f-8c63-e01deb64cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 10\n",
    "batch_size = 4\n",
    "\n",
    "inputs, targets = create_batches(encoded_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8bddd6e-6fa2-4010-866b-8e2b670a538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=embed_size, \n",
    "                                          nhead=num_heads, \n",
    "                                          num_encoder_layers=num_layers, \n",
    "                                          num_decoder_layers=num_layers, \n",
    "                                          dim_feedforward=hidden_size, \n",
    "                                          dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * np.sqrt(src.size(1))\n",
    "        src = self.positional_encoding(src)\n",
    "        \n",
    "        tgt = self.embedding(tgt) * np.sqrt(tgt.size(1))\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c8c818-63c1-4b18-923f-6c87690d19c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [0/20], Loss: 3.8986\n",
      "Epoch [2/20], Step [0/20], Loss: 2.9302\n",
      "Epoch [3/20], Step [0/20], Loss: 2.6717\n",
      "Epoch [4/20], Step [0/20], Loss: 2.3375\n",
      "Epoch [5/20], Step [0/20], Loss: 2.1031\n",
      "Epoch [6/20], Step [0/20], Loss: 1.8486\n",
      "Epoch [7/20], Step [0/20], Loss: 1.5706\n",
      "Epoch [8/20], Step [0/20], Loss: 1.3836\n",
      "Epoch [9/20], Step [0/20], Loss: 1.1733\n",
      "Epoch [10/20], Step [0/20], Loss: 1.0094\n",
      "Epoch [11/20], Step [0/20], Loss: 0.9424\n",
      "Epoch [12/20], Step [0/20], Loss: 0.7804\n",
      "Epoch [13/20], Step [0/20], Loss: 0.6759\n",
      "Epoch [14/20], Step [0/20], Loss: 0.5936\n",
      "Epoch [15/20], Step [0/20], Loss: 0.5737\n",
      "Epoch [16/20], Step [0/20], Loss: 0.4886\n",
      "Epoch [17/20], Step [0/20], Loss: 0.4456\n",
      "Epoch [18/20], Step [0/20], Loss: 0.3968\n",
      "Epoch [19/20], Step [0/20], Loss: 0.3552\n",
      "Epoch [20/20], Step [0/20], Loss: 0.3694\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the model\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate model, loss function and optimizer\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, inputs.shape[1], sequence_length):\n",
    "        # Prepare inputs and targets\n",
    "        input_batch = torch.tensor(inputs[:, i:i + sequence_length], dtype=torch.long)\n",
    "        target_batch = torch.tensor(targets[:, i:i + sequence_length], dtype=torch.long)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_batch, target_batch[:, :-1])\n",
    "        loss = criterion(output.reshape(-1, vocab_size), target_batch[:, 1:].reshape(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % (sequence_length * 10) == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Step [{i}/{inputs.shape[1]}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff04eda-5938-47d5-a4f7-0f5ecb8baf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prime, vocab_to_int, int_to_vocab, max_len=100, top_k=5):\n",
    "    model.eval()\n",
    "    input_sequence = [vocab_to_int[word] for word in prime.lower().split()]\n",
    "    input_sequence = torch.tensor([input_sequence], dtype=torch.long)\n",
    "    \n",
    "    generated_sequence = input_sequence.tolist()[0]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        tgt_input = torch.tensor([generated_sequence], dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_sequence, tgt_input)\n",
    "        \n",
    "        p = torch.nn.functional.softmax(output[:, -1, :], dim=1).data\n",
    "        p, top_token = p.topk(top_k)\n",
    "        top_token = top_token.squeeze().cpu().numpy()\n",
    "        p = p.squeeze().cpu().numpy()\n",
    "        \n",
    "        next_token = np.random.choice(top_token, p=p/p.sum())\n",
    "        generated_sequence.append(next_token)\n",
    "        \n",
    "        if next_token == vocab_to_int['<eos>']:\n",
    "            break\n",
    "    \n",
    "    generated_words = [int_to_vocab[token] for token in generated_sequence]\n",
    "    return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5eeff15-a136-4295-ac87-6691ad228c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bird the bird the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the bird flew over the tree. the bird flew over the tree. the bird flew over the tree. the bird flew over the bird flew over the tree. the bird liked to play near the tree. the bird flew over the bird flew over the bird flew over the bird flew over the bird\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example of generating text after training\n",
    "generated_text = generate_text(model, \n",
    "                               prime='The bird', \n",
    "                               vocab_to_int=vocab_to_int, \n",
    "                               int_to_vocab=int_to_vocab, \n",
    "                               max_len=100,\n",
    "                               top_k=2)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a89492-7473-472e-b9fb-f4dc0e0d32a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
