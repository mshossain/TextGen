{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f315b540-65e1-4192-b676-2d587eea9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d905b34e-4c24-421f-aa70-da2e6fed3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading text file\n",
    "with open('input_text.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a6ede9-aa75-4f92-aa08-5cd0828bf08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(sorted_vocab, 1)}\n",
    "    int_to_vocab = {ii: word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    encoded = [vocab_to_int[word] for word in words]\n",
    "    return encoded, vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b10c468-8aa4-4687-9553-ac955fdb09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, vocab_to_int, int_to_vocab = preprocess(text)\n",
    "vocab_size = len(vocab_to_int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191419a2-7533-4542-b7e8-1ec766dd8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded: List or NumPy array of integers. This is the tokenized text.\n",
    "# sequence_length: Integer. The length of each input sequence. \n",
    "# batch_size: Integer. The number of sequences that will be \n",
    "# processed together in parallel (in a batch).\n",
    "# Example:\n",
    "    #encoded = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    #sequence_length = 2\n",
    "    #batch_size = 2\n",
    "def create_batches(encoded, sequence_length, batch_size):\n",
    "    total_length = len(encoded)\n",
    "    n_batches = total_length // (batch_size * sequence_length)\n",
    "    encoded = encoded[:n_batches * batch_size * sequence_length]\n",
    "    input_data = np.array(encoded)\n",
    "    target_data = np.roll(input_data, -1)\n",
    "    \n",
    "    inputs = input_data.reshape((batch_size, -1))\n",
    "    targets = target_data.reshape((batch_size, -1))\n",
    "\n",
    "    # inputs: NumPy array, shape (batch_size, num_batches * sequence_length).\n",
    "    # Example: \n",
    "        # inputs = [[1, 2, 3, 4], [5, 6, 7, 8]]\n",
    "        # targets = [[2, 3, 4, 5], [6, 7, 8, 1]]\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa88945-cf45-46dc-a5a3-1a364c1f819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 5\n",
    "batch_size = 4\n",
    "\n",
    "inputs, targets = create_batches(encoded_text, sequence_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe0e9bd-2b63-4cbf-9fee-2381f501ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        # Batch size: out.size(0)\n",
    "        # Sequence length: out.size(1)\n",
    "        # Hidden size: out.size(2)\n",
    "        out = self.fc(out.reshape(out.size(0) * out.size(1), out.size(2)))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e50f960-2811-4b43-9c1a-fc75e95a16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the model\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531a018f-a6b8-4f9a-ad2c-bf4d8e0955a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, loss function and optimizer\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d7c1b8-cafa-41d3-9825-df4eb1ee91e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [0/20], Loss: 3.6971\n",
      "Epoch [2/20], Step [0/20], Loss: 3.3524\n",
      "Epoch [3/20], Step [0/20], Loss: 3.1618\n",
      "Epoch [4/20], Step [0/20], Loss: 3.0255\n",
      "Epoch [5/20], Step [0/20], Loss: 2.7335\n",
      "Epoch [6/20], Step [0/20], Loss: 2.3177\n",
      "Epoch [7/20], Step [0/20], Loss: 1.8860\n",
      "Epoch [8/20], Step [0/20], Loss: 1.4625\n",
      "Epoch [9/20], Step [0/20], Loss: 1.1086\n",
      "Epoch [10/20], Step [0/20], Loss: 0.7997\n",
      "Epoch [11/20], Step [0/20], Loss: 0.5802\n",
      "Epoch [12/20], Step [0/20], Loss: 0.4291\n",
      "Epoch [13/20], Step [0/20], Loss: 0.3400\n",
      "Epoch [14/20], Step [0/20], Loss: 0.2538\n",
      "Epoch [15/20], Step [0/20], Loss: 0.2298\n",
      "Epoch [16/20], Step [0/20], Loss: 0.1829\n",
      "Epoch [17/20], Step [0/20], Loss: 0.1615\n",
      "Epoch [18/20], Step [0/20], Loss: 0.1344\n",
      "Epoch [19/20], Step [0/20], Loss: 0.1199\n",
      "Epoch [20/20], Step [0/20], Loss: 0.1149\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], sequence_length):\n",
    "        # Prepare inputs and targets\n",
    "        input_batch = torch.tensor(inputs[:, i:i + sequence_length], dtype=torch.long)\n",
    "        target_batch = torch.tensor(targets[:, i:i + sequence_length], dtype=torch.long)\n",
    "        \n",
    "        # Detach hidden state to prevent backpropagating through entire history\n",
    "        hidden = tuple([each.detach() for each in hidden])\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, hidden = model(input_batch, hidden)\n",
    "        loss = criterion(output, target_batch.view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % (sequence_length * 10) == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Step [{i}/{inputs.shape[1]}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041f79cc-284a-474f-b644-3f8d35e8f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, word, vocab_to_int, int_to_vocab, hidden=None, top_k=5):\n",
    "    # Convert word to integer (token)\n",
    "    x = np.array([[vocab_to_int[word]]])\n",
    "    inputs = torch.tensor(x, dtype=torch.long)\n",
    "    \n",
    "    # Ensure hidden state is detached from the computational graph\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    out, hidden = model(inputs, hidden)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    p = torch.nn.functional.softmax(out, dim=1).data\n",
    "    p, top_tokens = p.topk(top_k)\n",
    "    \n",
    "    # Convert to numpy arrays for processing\n",
    "    top_tokens = top_tokens.numpy().squeeze()\n",
    "    p = p.numpy().squeeze()\n",
    "    \n",
    "    # Randomly select the next word based on probabilities\n",
    "    next_word_token = np.random.choice(top_tokens, p=p/p.sum())\n",
    "    \n",
    "    # Return the predicted word and the hidden state\n",
    "    return int_to_vocab[next_word_token], hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7897ca59-e61b-45df-ab56-271e12eb6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, prime='the', top_k=5):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Start with the prime words\n",
    "    words = prime.lower().split()  # Split prime into words\n",
    "    \n",
    "    # Initialize the hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    # Pass through the prime words to prime the model\n",
    "    for word in words:\n",
    "        if word in vocab_to_int:  # Ensure the word exists in the vocab\n",
    "            predicted_word, hidden = \\\n",
    "            predict(model, word, vocab_to_int, int_to_vocab, hidden, top_k)\n",
    "        else:\n",
    "            print(f\"Warning: Word '{word}' not found in vocabulary.\")\n",
    "            return ' '.join(words)  \n",
    "            # Return the prime words if a word is missing from vocab\n",
    "    \n",
    "    # Add the first predicted word\n",
    "    words.append(predicted_word)\n",
    "    \n",
    "    # Generate `size` number of additional words\n",
    "    for _ in range(size):\n",
    "        word, hidden = predict(model, words[-1], vocab_to_int, int_to_vocab, hidden, top_k)\n",
    "        words.append(word)\n",
    "    \n",
    "    # Join words into a single string and return the generated text\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97ed8291-c2d7-462e-9060-4f328cd3edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bird sang in the tree. the bird liked to play near the tree. the tree was and the bird the bird the bird the over the bird liked to play near the tree. the tree was tall and the bird the bird the bird the over the bird liked to play near the tree. the tree was tall and the bird the bird the bird the bird the trees. the bird the bird the liked to play near the the tree. the tree was tall and the bird the bird the bird the trees. the bird the bird the liked to play near the tree. the tree was tall and the bird the bird the bird the bird the bird liked to play near the tree. the tree was tall and the bird the bird the bird liked to play near the tree. the tree was tall and the bird the bird the bird the bird the over the bird liked to near the tree. the tree was tall and the bird the bird the bird the over the bird liked to play near the tree. the tree was tall and the bird the bird the bird the bird the trees. the\n"
     ]
    }
   ],
   "source": [
    "# Example of generating text after training\n",
    "generated_text = sample(model, 200, prime='The bird', top_k=3)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a45d6a-f83a-47e4-b13b-2a5b8ddf96bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fed1b-49a0-4b37-a671-4ba71dbe3a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
